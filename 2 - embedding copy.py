### Form√•l
# Scriptet indl√¶ser json filerne, embedder dem og gemmer dem s√• i et vector_store format
# Kr√¶ver en API key

import json
import os

##Indl√¶ser data fra json-databasen i forrige script
documents = []

with open("json_database/rag_chunks.jsonl", "r") as f:
    for line in f:
        item = json.loads(line) #loads fordi det er et jsonl format i stedet for json
        documents.append(item) #henter fra json


##konverterer fra json til document-objekter, som kan bruges i langchain

#LangChains Document-klasse er en struktur, der indeholder: 
# page_content (str): Det faktiske indhold, man vil give til modellen.
#metadata (dict): Ekstra info, fx kilde, titel, sidetal, etc.
from langchain.docstore.document import Document
docs = [Document(page_content=d["content"], metadata=d["metadata"]) for d in documents] #binder det sammen i et langchain dokument


#Ops√¶tter til API
from dotenv import load_dotenv
load_dotenv() ##henter key
print("API key:", os.getenv("OPENAI_API_KEY"))

#ops√¶tter embedding-model til openAI
from langchain_openai import OpenAIEmbeddings
import openai
embedding_model = OpenAIEmbeddings()

#Gemmer det embeddede materiale som vector store med FAISS. 
from langchain.vectorstores import FAISS 
db = FAISS.from_documents(docs, embedding_model)
db.save_local("vector_store")

##########################################################
##test af embedding-modellen
# Brugersp√∏rgsm√•l / prompt
query = "Hvad har Rigsrevisionen konkluderet i beretningen om palliation?"
aar_tidligst = 2022
aar_senest = 2025
dokument_type = "beretning"

# S√∏g i FAISS efter de mest relevante chunks
##her kunne man l√¶gge et filter ind med search_kwargs("filter"), men den filtrerer f√∏rst efter embedding-modellen er k√∏rt.

#manuelt filter f√∏r embedding-model og opretter ny vector-databasen
filtrede_doks = [doc for doc in docs if (
                     aar_tidligst <= doc.metadata.get("year", 0) <= aar_senest and
                     doc.metadata.get("dokumenttype") == dokument_type)] 
filtreret_db = FAISS.from_documents(filtrede_doks, embedding_model)

#embedding-modellen s√∏ger efter k matches
results = db.similarity_search(query, k=5)  # k = hvor mange top-resultater du vil have

# Udskriv resultaterne
for i, doc in enumerate(results, 1):
    print(f"\n--- Match {i} ---")
    print("üìÑ Indhold:")
    print(doc.page_content[:500])  # begr√¶ns l√¶ngden
    print("üßæ Metadata:", doc.metadata)